# Data Structures and Data Management

# Chapter 1 - Introduction

### Common facts we will use:
**Arithmetic sequence:** $\sum_{i=0}^{n-1}(a+di) = na + \frac{dn(n-1)}{2} \in \Theta (n^2)$ if $d \neq 0$
    - we will especially use $\sum_{i=0}^{n-1}i = \frac{n(n-1)}{2} \in \Theta (n^2)$
**Geometric sequence:** $\sum_{i=0}^{n-1}ar^i  =$
- if $r > 1$ then $a \frac{r^n-1}{r-1} \in \Theta (r^{n-1})$ 
- if $r = 1$ then $na \in \Theta (n)$
- if $0 < r < 1$ then $a \frac{1-r^n}{1-r} \in \Theta (1)$ 

**Harmonic sequence:** $H_n := \sum_{i=1}^{n} \frac{1}{i} = ln(n) + \gamma + o(1) \in \Theta (log n)$
- where $\gamma \approx 0.57721$ 
- $\sum_{i=1}^{n} \frac{1}{i^2} = \frac{\pi^2}{6} \in \Theta (1)$ and 
- $\sum_{i=1}^{n} i^k \in \Theta (n^{k+1})$ for $k \geq 0$

**Logarithms:**
- c = $log_b(a)$ means $b^c=a$
- $log(a)$ in this course always means $log_2(a)$
- $log(a \cdot c) = log(a) + log(c)$, $log(a^c) = clog(a)$
- $log_b(a) = \frac{log_c(a)}{log_c(b)} = \frac{1}{log_a(b)}$, $a^{log_b(c)} = c^{log_b(a)}$
- $ln(x)$ = natural log = $log_e(x)$, $\frac{d}{dx}ln(x) = \frac{1}{x}$
- concavity: $\lambda log(x) + (1-\lambda)log(y) \leq log(\lambda x + (1-\lambda ) y)$ for $0 \leq \lambda \leq 1$

**Factorial:**
- $n! = n(n-1) \cdots 2 \cdot 1$ = number of ways to permute n elements
- $log(n!) = log(n) + log(n-1) + \cdots + log(2) + log(1) \int \Theta (n log(n))$

**Probability and moments:**
- Lineartiy of expectation: $E[aX] = aE[X], E[X+Y] = E[X] + E[Y]$, where $X, Y$ are random variables and $E[\cdot]$ denotes the expected value
- If $X$ is a random variable that takes on only non-negative integer values, then $E[X] = \sum_{i=0}^\infty i \cdot P(X = i) = \sum_{i=1}^\infty P(X \geq i)$

## 1.2 - Definitions and Assumptions 
### Problems
Formal definition of *problem* in computer science is: it is a *language* i.e., a (possibly infinite) subset of words 
of an alphabet, and we want to know whether a given word belonds to the language. In this course we 
don't need this definition and instead we say a problem consists of a description of all possible 
inputs ("*instances*") and a requirement on the desired output.             
The course focuses on two problems: 
- *Sorting Problem*
- *Search Problem*
    - *structured search:* we *search by keys* where eat item is usually a *key-value-pair*
    - *unstructured search:* we do not know what type of search will be happening
Searching will occupy the bulk of this course. 

### Algorithms
Precise definition of algorithm is that it is a Turing machine.         
Our definition: an algorithm is a step-by-step process that caries out a series of computations on the given input. 
- it is correct if every possible isntance the algorithm obtains the correct output (such algorithm *solves* the problem)

### Computer model 
We are using the *Random Acecss Machine* (RAM) *model*: 
- Consists of memory cells which store one item number (cells are commonly called **computer words** but we will use words to refer to strings so we stick with cells)
- There is no limit on the number of memory cells that the algorithm can use 
    - this is not too realistic, but in the age of cheap cloud memory it's not too unrealistic
- Memory cell is big enough to store an integer we deal with 
    - this is not realistic: The computer either stores 32 or 64 bit numbers, but the RAM model is a model not a computer so if a number is too big it should be treated as a word not a number. 
- Any *primitive operation* should take the same amount of time. 
    - addition, subtraction, multiplication, division, taking mod, rounding, comparison, and control operations of accessing a memory cell, calling a sub-routine, entering a loop, breaking or returning. 
    - assuming these all take the same time is unrealistic. 

### Algorithm analysis
*Run-time* of an algorithm is the number of memory accesses plus the number of primitive operations.        

The size of an input (instance) $I$ is the number of memory cells it takes. 
- the run-time is measured relative to this size

We consider the worst case to calculate the run-time (gives a guaranteed upper bound). 
- this sometimes doesn't make sense to use, but computing average run-time is more challenging

## 1.3 - Asymptotic Notation

**Definition 1:** Let $f(n), g(n)$ be two functions. We say that $f(n) \in O(g(n))$ if there exists $c > 0$ and $n_0 \geq 0$ such that $|f(n)| \leq c|g(n)|$ for all $n \geq n_0$. 
- we sat f  is in big-O of g (technically we should always mention "asymptotically")
- we abbreviate and say for sufficiently large n

The absolute value signs can usually be ignored in this course as we're dealing with large positive integers. 

### Tight asymptotic bounds
Regular big O allows too much freedome (some function in $O(n^2)$ is also in $O(n^100)$)

We want to say "the run-time is quadratic and we can't get any better than that"
- the second part of the statement is expressed with "big-Omega" denoted $\Omega$ 
    - also called *asymptotic lower bound*

**Definition 2:** Let $f(n), g(n)$ be two functions. We say $f(n) \in \Omega (g(n))$ if there exists $c > 0$ and $n_0 \geq 0$ such that $|f(n) \geq c|g(n)|$ for all $n \geq n_0$
- note this definition is exactly the same as big-O except that the inequality is reversed

**Definition 3:** Let $f(n), g(n)$ be two functions. We say $f(n) \in \Theta (g(n))$ if $f(n) \in O(g(n))$ and $f(n) \in \Omega(g(n))$. So there exist constants $n_0 \geq 0$ and $c_1, c_2 > 0$ such that $c_1|g(n)| \leq |f(n)| \leq c_2|g(n)|$ for all $n \geq n_0$
- "f(n) is asymptotically the same as g(n)", "the run-time is $\Theta(n^2)$", "the run-time is $O(n^2)$ and this is tight"
- for meaningful bounds we should always use $\Theta$

### Growth rates
$T(n)$ is the run-time of some algorithm. 
Cmmon growth rates have names: 
- *constant* if $T(n) \in \Theta(1)$ we also say $T(n) \approx c$ for some constant c which is short for "$c \leq T(n) \leq c'$" for some constants $c$,$c'$
- *logarithmic* if $T(n) \in \Theta(log(n))$ i.e, $T(n) \approx c \cdot log(n)$ 
- *linear* if $T(n) \in \Theta(n)$ i.e, $T(n) \approx cn$
- *linearthmic* if $T(n) \in \Theta (n \cdot log(n))$ i.e, $T(n) \approx c \cdot n \cdot log(n)$ (this name is rarely used but the run-time is very common)
- *quadratic* if $T(n) \in \Theta (n^2)$ i.e, $T(n) \approx c \cdot n^2$
- *polynomial* if $T(n) \in \Theta (n^d)$ 
- *exponential* if $T(n) \in \Theta (b^n)$ for some const $b>1$ (very bad run-time)

### Friends of $O$ and $\Omega$

**Definition 4:** Let $f(n), g(n)$ be two functions. We say that $f(n) \in o(g(n))$ if for any constant $c > 0$ there exists an $n_0 \geq 0$ such that $|f(n) \leq c|g(n)|$ for all $n \geq n_0$            
We say that $f(n) \in \omega (g(n))$ if for any constant $c > 0$ there exists an $n_0 \geq 0$ such that $|f(n)| \geq c|g(n)|$ for all $n \geq n_0$. 
- "little-o" = "asymptotically strictly smaller" 
- "little-omega" = "asymptotically strictly larger" 

We rarely use little-omega in this course but we use little-o to state impossibilities and to express lower order terms if we care about constant "number of comparisons is $2n + o(n)$"

With big-O you hold (you get to choose one) *one* constant $c$ and show that there exists $n_0$. For little-o you must show for any c (particularly any small c) there exists $n_0$

Usually we do not prove from first principle (definition) little-o and little-omega propositions (we use limits). 

### Rules for asymptotic notation 
#### Some basic rules (for non-negative functions)
**Identity:** $f(n) \in \Theta(f(n))$
- prove using c = 1 

**Constant multiplication:** $K \cdot f(n) \in  \Theta(f(n))$
- prove using c = K

**Transitivity:** If $f(n) \in O(g(n))$ and $g(n) \in O(h(n))$ then $f(n) \in O(h(n))$ the same holds for $\Omega$
This implies **Dominance:**
    -  If $f(n)$ in $O(g(n))$, and $g(n) \leq h(n)$ for all sufficiently large n, then $f(n) \in O(h(n))$
    - the same holds for $\Omega$ but $g(n) \geq h(n)$ for sufficiently large n 

**Addition:** If $f(n) \in O(g(n))$ and $f'(n) \in O(g'(n))$ then $f(n)+f'(n) \in \O(g(n) + g'(n))$
- the same holds for $\Omega$

**Maximum-rule for $O$:** If a function $h(n)$ belongs to $O(f(n) + g(n))$ then it belongs to $O(max\{f(n), g(n)\})$. Here $max\{f(n), g(n)\}$ denotes the function that takes the pointwise
maximum, i.e., for every $n$ it takes on whichever value of $f(n)$ and $g(n)$ is the bigger one.

**Maximum-rule for $O$:** If a function $h(n)$ belongs to $\Omega(f(n) + g(n))$ then it belongs to $\Omega(max\{f(n),g(n)\})$

**Warning about arithmetic** 
Big-O notation refers to sets **do not** perform arithmetic on sets.        
We say $O(n) \subset O(n^2)$ not $O(n) < O(n^2)$
- you cannot add sets with +


**Lemma 1.1:** Let f(n), g(n) be two functions that satisfy $f(n), g(n) > 0$ for sufficiently large n$ assume that the limit $lim_{n \rightarrow \infty} \frac{f(n)}{g(n)}$ exists and call it $L$. Then ,       
$f(n) \in$
- $o(g(n))$ if $L = 0$
- $O(g(n))$ if $L < \infty$
- $\Omega(g(n))$ if $L > 0$
- $\omega(g(n))$ if $L = \infty$

**Lemma 1.2:** let f(n), g(n) be two functions that satisfy $f(n), g(n) > 0$ for sufficiently large n. Then $f(n) \in o(g(n))$ if and only if $lim_{n \rightarrow \infty} \frac{f(n)}{g(n)}$ exists and equals 0.

**Polynomial rule:** If $f$ is a polynomial with leading term $c_d \cdot n ^ d$ then $f(n) \in \Theta (n^d)$

**Logarithm rule:** 
- $log_b(n) \in \Theta(log (n))$ for $b > 1$
    - all logarithms grow the same asymptotically
- $log(n) \in o(n)$
    - logarithms grow much slower than linear functions

**Relationships between asymptotic notations:** A 

**Lemma 1.4:** Let $f(n), g(n)$ be two functions with $f(n),g(n) > 0$ for sufficiently large n. If $f(n) \in \Omega(g(n))$ then $f(n) \notin o(g(n))$

## 1.4 - Algorithm analysis revisted
How do we describe an algorithm? 

#### Step 1. Describe the algorithm idea 
Should be in natural words with 1-2 sentences and/or a picture that describes the important insights. 

For example (insertion-sort): Keep a sorted part, and repeatedly insert one more item into its correct place in it. 

#### Step 2. Give a detailed description
This typically means pseudo-code. Should be descriptive enough than an experienced programmer can convert to code without much thinking. 
```
// Algorithm 1.4: insertion-sort(A, n <- A.size)
Input : Array A of size at least n
for i <- 1 to n-1 do 
    j <- i 
    while j > 0 and A[j] > A[j-1] do 
        swap A[j] and A[j-1]
        j <- j - 1
```

#### Step 3. Argue correctness
loop-invariants, and stuff from cs245. Proofs for this course don't need to be as formal as cs245. 

#### Step 4. Analyze the efficiency 
Usually the worst-case scenario, but if average is better than worst case this should be stated. Both should be using $\Theta$-notation (tight bounds). 
- should also analyze auxiliary space 

### Comparing algorithms 
We consider one algorithm to be worse if its run-time is asymptotically strictly smaller (little-o). 
- if it is a tie we use space or other other run-time cases 
- using worst-case is far from a perfect solution (O(n^3) > O(10^20 * n^2)) 

### Case study: Merge-sort 
**Step 1:** The idea is to split input into roughly two halves and sort them recursively then merge the two sorted parts together. 

**Step 2:** Pseudo-code
- we need two fragments, the merge-routine and the merge-sort

**Algorithm 1.5:** merge-sort(A, n = A.size(), l = 0, r = n-1, s = NIL)
```
Input : Array a of size at least n, 0<=l<=r<=n-1
        (possibly uninitialized) array S of size n 
if S is NIL then initialize it as array S[0..n-1]
if (r <= l) then 
    return 
else 
    m = floor((r + l) / 2) 
    merge-sort(A,n,l,m,S)
    merge-srot(A,n,m+1, r, S)
    merge(A, l, m, r, S)
```

**Algorithm 1.5:** merge(A, l, m, r, S)
```
Input : A[0..n-1] is an array, A[l..m] is sorted, A[m+1..r] is sorted. 
        S[0..n-1] is an array 
copy A[l..r] into S[l..r]
int il = l 
int ir = m + 1 
for (k = l; k <= r; k++) do 
    if il > m then A[k] = S[ir++]
    else if ir > r then A[k] = S[il++] 
    else if S[il] <= S[ir] then A[k] = S[il++]
    else A[k] = S[ir++]
```

**Step 3:** Correctness. First argue that merge achieves a sorted array afterwards. This 
- the two parts of the array were sorted before and copy over the smaller item. We split the array in two parts and sort each recursively and since merge is correct the end result is sorted. 

We should also brieful discuss that the recursion terminates. Show that the two subarray are smaller and we reach r>=l eventually. 

**Step 4:** Analyze the efficiency 

Since merge-sort is recursive we have a recursive definition for the worst-case $T(n) \approx 2T(n/2) + cn$ (and c if <=1). Sincethe growth of most algorithms is smooth relative to n we just assume that n is divisible as needed (don't worry about n/2). 
- this resolves to $T(n) \in \Theta(n \cdot log(n))$

There is a table for finding $\Theta$ for recursive expressions. 

We should also discuss space 
- merge-sort allocates another array of size in so it takes $$\Theta(n)$$ auxiliary space. 
    - the function calls in the stack take $\Theta (log(n))$ calls but they are dominated by the array allocation 

## 1.5 - Take-home messages
- To describe an algorithm we state the main idea, give detailed description (pseudo-code), argue correctness, analyze run-time and ausxiliary space.  
- To compare to algorithm we compare worst-case run-times. Break ties by other considerations such as space and/or best-case run-time. 
- Asymptotic notation is a way to express bounds without dealing with constants. 
- Bounds on run-time and space should always be states as **tight** bounds 
Algorithm merge-sort solves the sorting problem in $\Theta(n log(n))$ run-time and uses $\Theta(n)$ auxiliary space.


