# Data Structures and Data Management

# Chapter 1 - Introduction

### Common facts we will use:
**Arithmetic sequence:** $\sum_{i=0}^{n-1}(a+di) = na + \frac{dn(n-1)}{2} \in \Theta (n^2)$ if $d \neq 0$
    - we will especially use $\sum_{i=0}^{n-1}i = \frac{n(n-1)}{2} \in \Theta (n^2)$
**Geometric sequence:** $\sum_{i=0}^{n-1}ar^i  =$
- if $r > 1$ then $a \frac{r^n-1}{r-1} \in \Theta (r^{n-1})$ 
- if $r = 1$ then $na \in \Theta (n)$
- if $0 < r < 1$ then $a \frac{1-r^n}{1-r} \in \Theta (1)$ 

**Harmonic sequence:** $H_n := \sum_{i=1}^{n} \frac{1}{i} = ln(n) + \gamma + o(1) \in \Theta (log n)$
- where $\gamma \approx 0.57721$ 
- $\sum_{i=1}^{n} \frac{1}{i^2} = \frac{\pi^2}{6} \in \Theta (1)$ and 
- $\sum_{i=1}^{n} i^k \in \Theta (n^{k+1})$ for $k \geq 0$

**Logarithms:**
- c = $log_b(a)$ means $b^c=a$
- $log(a)$ in this course always means $log_2(a)$
- $log(a \cdot c) = log(a) + log(c)$, $log(a^c) = clog(a)$
- $log_b(a) = \frac{log_c(a)}{log_c(b)} = \frac{1}{log_a(b)}$, $a^{log_b(c)} = c^{log_b(a)}$
- $ln(x)$ = natural log = $log_e(x)$, $\frac{d}{dx}ln(x) = \frac{1}{x}$
- concavity: $\lambda log(x) + (1-\lambda)log(y) \leq log(\lambda x + (1-\lambda ) y)$ for $0 \leq \lambda \leq 1$

**Factorial:**
- $n! = n(n-1) \cdots 2 \cdot 1$ = number of ways to permute n elements
- $log(n!) = log(n) + log(n-1) + \cdots + log(2) + log(1) \int \Theta (n log(n))$

**Probability and moments:**
- Lineartiy of expectation: $E[aX] = aE[X], E[X+Y] = E[X] + E[Y]$, where $X, Y$ are random variables and $E[\cdot]$ denotes the expected value
- If $X$ is a random variable that takes on only non-negative integer values, then $E[X] = \sum_{i=0}^\infty i \cdot P(X = i) = \sum_{i=1}^\infty P(X \geq i)$

## 1.2 - Definitions and Assumptions 
### Problems
Formal definition of *problem* in computer science is: it is a *language* i.e., a (possibly infinite) subset of words 
of an alphabet, and we want to know whether a given word belonds to the language. In this course we 
don't need this definition and instead we say a problem consists of a description of all possible 
inputs ("*instances*") and a requirement on the desired output.             
The course focuses on two problems: 
- *Sorting Problem*
- *Search Problem*
    - *structured search:* we *search by keys* where eat item is usually a *key-value-pair*
    - *unstructured search:* we do not know what type of search will be happening
Searching will occupy the bulk of this course. 

### Algorithms
Precise definition of algorithm is that it is a Turing machine.         
Our definition: an algorithm is a step-by-step process that caries out a series of computations on the given input. 
- it is correct if every possible isntance the algorithm obtains the correct output (such algorithm *solves* the problem)

### Computer model 
We are using the *Random Acecss Machine* (RAM) *model*: 
- Consists of memory cells which store one item number (cells are commonly called **computer words** but we will use words to refer to strings so we stick with cells)
- There is no limit on the number of memory cells that the algorithm can use 
    - this is not too realistic, but in the age of cheap cloud memory it's not too unrealistic
- Memory cell is big enough to store an integer we deal with 
    - this is not realistic: The computer either stores 32 or 64 bit numbers, but the RAM model is a model not a computer so if a number is too big it should be treated as a word not a number. 
- Any *primitive operation* should take the same amount of time. 
    - addition, subtraction, multiplication, division, taking mod, rounding, comparison, and control operations of accessing a memory cell, calling a sub-routine, entering a loop, breaking or returning. 
    - assuming these all take the same time is unrealistic. 

### Algorithm analysis
*Run-time* of an algorithm is the number of memory accesses plus the number of primitive operations.        

The size of an input (instance) $I$ is the number of memory cells it takes. 
- the run-time is measured relative to this size

We consider the worst case to calculate the run-time (gives a guaranteed upper bound). 
- this sometimes doesn't make sense to use, but computing average run-time is more challenging

## 1.3 - Asymptotic Notation

**Definition 1:** Let $f(n), g(n)$ be two functions. We say that $f(n) \in O(g(n))$ if there exists $c > 0$ and $n_0 \geq 0$ such that $|f(n)| \leq c|g(n)|$ for all $n \geq n_0$. 
- we sat f  is in big-O of g (technically we should always mention "asymptotically")
- we abbreviate and say for sufficiently large n

The absolute value signs can usually be ignored in this course as we're dealing with large positive integers. 

### Tight asymptotic bounds
Regular big O allows too much freedome (some function in $O(n^2)$ is also in $O(n^100)$)

We want to say "the run-time is quadratic and we can't get any better than that"
- the second part of the statement is expressed with "big-Omega" denoted $\Omega$ 
    - also called *asymptotic lower bound*

**Definition 2:** Let $f(n), g(n)$ be two functions. We say $f(n) \in \Omega (g(n))$ if there exists $c > 0$ and $n_0 \geq 0$ such that $|f(n) \geq c|g(n)|$ for all $n \geq n_0$
- note this definition is exactly the same as big-O except that the inequality is reversed

**Definition 3:** Let $f(n), g(n)$ be two functions. We say $f(n) \in \Theta (g(n))$ if $f(n) \in O(g(n))$ and $f(n) \in \Omega(g(n))$. So there exist constants $n_0 \geq 0$ and $c_1, c_2 > 0$ such that $c_1|g(n)| \leq |f(n)| \leq c_2|g(n)|$ for all $n \geq n_0$
- "f(n) is asymptotically the same as g(n)", "the run-time is $\Theta(n^2)$", "the run-time is $O(n^2)$ and this is tight"
- for meaningful bounds we should always use $\Theta$

### Growth rates
$T(n)$ is the run-time of some algorithm. 
Cmmon growth rates have names: 
- *constant* if $T(n) \in \Theta(1)$ we also say $T(n) \approx c$ for some constant c which is short for "$c \leq T(n) \leq c'$" for some constants $c$,$c'$
- *logarithmic* if $T(n) \in \Theta(log(n))$ i.e, $T(n) \approx c \cdot log(n)$ 
- *linear* if $T(n) \in \Theta(n)$ i.e, $T(n) \approx cn$
- *linearthmic* if $T(n) \in \Theta (n \cdot log(n))$ i.e, $T(n) \approx c \cdot n \cdot log(n)$ (this name is rarely used but the run-time is very common)
- *quadratic* if $T(n) \in \Theta (n^2)$ i.e, $T(n) \approx c \cdot n^2$
- *polynomial* if $T(n) \in \Theta (n^d)$ 
- *exponential* if $T(n) \in \Theta (b^n)$ for some const $b>1$ (very bad run-time)

### Friends of $O$ and $\Omega$

**Definition 4:** Let $f(n), g(n)$ be two functions. We say that $f(n) \in o(g(n))$ if for any constant $c > 0$ there exists an $n_0 \geq 0$ such that $|f(n) \leq c|g(n)|$ for all $n \geq n_0$            
We say that $f(n) \in \omega (g(n))$ if for any constant $c > 0$ there exists an $n_0 \geq 0$ such that $|f(n)| \geq c|g(n)|$ for all $n \geq n_0$. 
- "little-o" = "asymptotically strictly smaller" 
- "little-omega" = "asymptotically strictly larger" 

We rarely use little-omega in this course but we use little-o to state impossibilities and to express lower order terms if we care about constant "number of comparisons is $2n + o(n)$"

With big-O you hold (you get to choose one) *one* constant $c$ and show that there exists $n_0$. For little-o you must show for any c (particularly any small c) there exists $n_0$

Usually we do not prove from first principle (definition) little-o and little-omega propositions (we use limits). 

### Rules for asymptotic notation 
#### Some basic rules (for non-negative functions)
**Identity:** $f(n) \in \Theta(f(n))$
- prove using c = 1 

**Constant multiplication:** $K \cdot f(n) \in  \Theta(f(n))$
- prove using c = K

**Transitivity:** If $f(n) \in O(g(n))$ and $g(n) \in O(h(n))$ then $f(n) \in O(h(n))$ the same holds for $\Omega$
This implies **Dominance:**
    -  If $f(n)$ in $O(g(n))$, and $g(n) \leq h(n)$ for all sufficiently large n, then $f(n) \in O(h(n))$
    - the same holds for $\Omega$ but $g(n) \geq h(n)$ for sufficiently large n 

**Addition:** If $f(n) \in O(g(n))$ and $f'(n) \in O(g'(n))$ then $f(n)+f'(n) \in \O(g(n) + g'(n))$
- the same holds for $\Omega$

**Maximum-rule for $O$:** If a function $h(n)$ belongs to $O(f(n) + g(n))$ then it belongs to $O(max\{f(n), g(n)\})$. Here $max\{f(n), g(n)\}$ denotes the function that takes the pointwise
maximum, i.e., for every $n$ it takes on whichever value of $f(n)$ and $g(n)$ is the bigger one.

**Maximum-rule for $O$:** If a function $h(n)$ belongs to $\Omega(f(n) + g(n))$ then it belongs to $\Omega(max\{f(n),g(n)\})$

**Warning about arithmetic** 
Big-O notation refers to sets **do not** perform arithmetic on sets.        
We say $O(n) \subset O(n^2)$ not $O(n) < O(n^2)$
- you cannot add sets with +


**Lemma 1.1:** Let f(n), g(n) be two functions that satisfy $f(n), g(n) > 0$ for sufficiently large n$ assume that the limit $lim_{n \rightarrow \infty} \frac{f(n)}{g(n)}$ exists and call it $L$. Then ,       
$f(n) \in$
- $o(g(n))$ if $L = 0$
- $O(g(n))$ if $L < \infty$
- $\Omega(g(n))$ if $L > 0$
- $\omega(g(n))$ if $L = \infty$

**Lemma 1.2:** let f(n), g(n) be two functions that satisfy $f(n), g(n) > 0$ for sufficiently large n. Then $f(n) \in o(g(n))$ if and only if $lim_{n \rightarrow \infty} \frac{f(n)}{g(n)}$ exists and equals 0.

**Polynomial rule:** If $f$ is a polynomial with leading term $c_d \cdot n ^ d$ then $f(n) \in \Theta (n^d)$

**Logarithm rule:** 
- $log_b(n) \in \Theta(log (n))$ for $b > 1$
    - all logarithms grow the same asymptotically
- $log(n) \in o(n)$
    - logarithms grow much slower than linear functions

**Relationships between asymptotic notations:** A 

**Lemma 1.4:** Let $f(n), g(n)$ be two functions with $f(n),g(n) > 0$ for sufficiently large n. If $f(n) \in \Omega(g(n))$ then $f(n) \notin o(g(n))$

## 1.4 - Algorithm analysis revisted
How do we describe an algorithm? 

#### Step 1. Describe the algorithm idea 
Should be in natural words with 1-2 sentences and/or a picture that describes the important insights. 

For example (insertion-sort): Keep a sorted part, and repeatedly insert one more item into its correct place in it. 

#### Step 2. Give a detailed description
This typically means pseudo-code. Should be descriptive enough than an experienced programmer can convert to code without much thinking. 
```
// Algorithm 1.4: insertion-sort(A, n <- A.size)
Input : Array A of size at least n
for i <- 1 to n-1 do 
    j <- i 
    while j > 0 and A[j] > A[j-1] do 
        swap A[j] and A[j-1]
        j <- j - 1
```

#### Step 3. Argue correctness

loop-invariants, and stuff from cs245. Proofs for this course don't need to be as formal as cs245. 

#### Step 4. Analyze the efficiency 
Usually the worst-case scenario, but if average is better than worst case this should be stated. Both should be using $\Theta$-notation (tight bounds). 
- should also analyze auxiliary space 

### Comparing algorithms 
We consider one algorithm to be worse if its run-time is asymptotically strictly smaller (little-o). 
- if it is a tie we use space or other other run-time cases 
- using worst-case is far from a perfect solution (O(n^3) > O(10^20 * n^2)) 

### Case study: Merge-sort 
**Step 1:** The idea is to split input into roughly two halves and sort them recursively then merge the two sorted parts together. 

**Step 2:** Pseudo-code
- we need two fragments, the merge-routine and the merge-sort

**Algorithm 1.5:** merge-sort(A, n = A.size(), l = 0, r = n-1, s = NIL)
```
Input : Array a of size at least n, 0<=l<=r<=n-1
        (possibly uninitialized) array S of size n 
if S is NIL then initialize it as array S[0..n-1]
if (r <= l) then 
    return 
else 
    m = floor((r + l) / 2) 
    merge-sort(A,n,l,m,S)
    merge-srot(A,n,m+1, r, S)
    merge(A, l, m, r, S)
```

**Algorithm 1.5:** merge(A, l, m, r, S)

```
Input : A[0..n-1] is an array, A[l..m] is sorted, A[m+1..r] is sorted. 
        S[0..n-1] is an array 
copy A[l..r] into S[l..r]
int il = l 
int ir = m + 1 
for (k = l; k <= r; k++) do 
    if il > m then A[k] = S[ir++]
    else if ir > r then A[k] = S[il++] 
    else if S[il] <= S[ir] then A[k] = S[il++]
    else A[k] = S[ir++]
```

**Step 3:** Correctness. First argue that merge achieves a sorted array afterwards. This 
- the two parts of the array were sorted before and copy over the smaller item. We split the array in two parts and sort each recursively and since merge is correct the end result is sorted. 

We should also brieful discuss that the recursion terminates. Show that the two subarray are smaller and we reach r>=l eventually. 

**Step 4:** Analyze the efficiency 

Since merge-sort is recursive we have a recursive definition for the worst-case $T(n) \approx 2T(n/2) + cn$ (and c if <=1). Sincethe growth of most algorithms is smooth relative to n we just assume that n is divisible as needed (don't worry about n/2). 
- this resolves to $T(n) \in \Theta(n \cdot log(n))$

There is a table for finding $\Theta$ for recursive expressions. 

We should also discuss space 
- merge-sort allocates another array of size in so it takes $$\Theta(n)$$ auxiliary space. 
    - the function calls in the stack take $\Theta (log(n))$ calls but they are dominated by the array allocation 

# 1.5 - Take-home messages
- To describe an algorithm we state the main idea, give detailed description (pseudo-code), argue correctness, analyze run-time and ausxiliary space.  
- To compare to algorithm we compare worst-case run-times. Break ties by other considerations such as space and/or best-case run-time. 
- Asymptotic notation is a way to express bounds without dealing with constants. 
- Bounds on run-time and space should always be states as **tight** bounds 

Algorithm merge-sort solves the sorting problem in $\Theta(n log(n))$ run-time and uses $\Theta(n)$ auxiliary space.

# Module 2 
## Stack ADT 
- push: inserting an item 
- pop: removing an item 

LIFO (last-in first-out) 

Extra operations: 
- size, isEmpty, top 

We can implement this with an array or a linked-list 

## Queue ADT 
- enqueue: inseting an item 
- dequeue: removing the least recently inserted item 

FIFO (first-in first-out) 
- size, isEmpty, front

circular array 
- keep track of front and end 

## Priority Queue
An ADT consisting of items each with a priority 

main operations 
- insert: inserting an item tagged with a priority 
- deleteMax: removing the item of highest priority 

deleteMax is also called 
- the priority is also called a **key**

Above we describe maximum-oriented priority queue, but it could be replaced with a minimum-oriented priority queue

Using a priority queue to sort 
```
PQ-Sort(A[0..n-1])
    initialize PQ to an empty priority queue 
    for k = 0 to n - 1 do 
        PQ.insert(A[k], A[k])
    for k = n - 1 down to 0 do 
        A[k] = PQ.deleteMax() 
```
- run-time is the sum of insertion and deletion 
    - will depend on how we implement PQ 

**note:** we usually assyme **dynamic arrays** i.e, expanding by doubling and so ammortized over all insertions this takes $O(1)$ extra time 

Suppose we start from an array A of length 1, we can do n insertions $n = 2^k$
- the cost of these insertions is 
- $O(\underset{\textrm{n times}}{1 + 1 + 1 + \cdots} + (1 + 2 + \cdots 2^{k-1} = 2^k -1 = n - 1)) = O(n)$
    - so we see that the additional cost of resizing does not change the insertion run-time from $O(n)$

**Realization 1:** unsorted arrays            

Using unsorted arrays 
- PQ-sort is $O(n^2)$ is **selection sort**

**Realization 2:** sorted arrays 
- insert $O(n)$
- deleteMax: $O(1)$

PQ-sort with sorted arrays is **insertion sort**, the runtime is $O(n^2)$

## Binary Heaps 

A **(binary) heap** is a certain type of binary tree 

A binary tree is either 
- empty or 
- consists of three parts 
    - a node and two binary trees (left subtree and right subtree) 

Terminology: root, leaf, parent, child, level, sibling, ancestor, descendant 

Any binary tree with n nodes has a height at least 
$$log(n+1) - 1 \in \Omega (log n)$$
- the height of a non-empty tree is the length of the longest path from the root to a ndoe 
- the height of the **empty** tree is $-1$

A **heap** is a binary tree with the following two properties: 
- **Structural property:** All levels of a heap are completely filled, except (possibly) for the last level. The filled items in the last level are **left-justified**
- **Heap-order property:** For any node i, the key of the parent of i is larger than or equal to the key of i 

The full name for this is a *max-oriented binary heap*. 

Lemma: The height of a heap is $\Theta (log (n))$ 

**Heap implementation:**

Heaps should **not** be stored as binary trees. 

We can store a heap in an array because of its structure property. 
- The root at A[0] then level 2 is A[1],A[2] and level 3 is A[3]...A[6]

- the **root** is at index 0 
- the **left child** of node i (if it exists) is node $2i - 1$
- the **right child** of node i (if it exists is node $2i + 1$
- the **parent** of node i is $\lfloor{\frac{i-1}{2}}\rfloor$
- the **last** node is $n-1$
 
we wrap these operations in root(), parent(i), last(n)

### Insert in Heaps 

- place the new key at the first free leaf (this must be done to maintain structure)
- the heap-order property might be violated: perform a fix-up 

```
fix-up(A,k)
k: an index corresponding to a node of the heap 
    while parent(k) exists and A[parent(k)] < A[k] do 
        swap A[k] and A[parent(k)]
        k = parent(k)
```
- the new item "bubbles up" until it reaches its correct place in the heap 
- runtime: $$O(\textrm{height of heap) = O(log(n))$$

### deleteMax in Heaps
```
fix-down(A, n, k): 
A: an array that stores a heap of size n 
k: an index corresponding to a node of the heap 
    while k is not a leaf do 
        // find the child with the larger key 
        j = left child of k 
        if (j is not last(n) and A[j+1] > A[j]) 
            j = j + 1
        if A[k] >= A[j] break 
            swap A[j] and A[k] 
            k = j 
```

### Priority queue using heaps 
```
insert(x) 
    increase size 
    l = last(size) 
    A[l] = x 
    fix-up(A, l) 

deleteMax()
    l = last(size) 
    swap A[root()] and A[l]
    decrease size 
    fix-down(A, size, root()) 
    return A[l] 
```
- insert and deleteMax: $O(logn)$

Time: $O(\textrm{ height of tree}) = O(log(n))$


## Sorting using heaps 
```
PQsortWithHeaps(A) 
    initialize H to an empty heap 
    for k = 0 to n - 1 do 
        H.insert(A[k]) (we just insert keys, no items) 
    for k = n - 1 down to 0 do 
        A[k] = H.deleteMax()
```
 
Can improve this with two simple tricks -> Heapsort 
1. Heaps can be built faster if we know all input in advance 
2. Can use the same array for input and heap -> $O(1)$$ auxiliary space 

### Building heaps with fix-up

Problem: given n items all at once build a heap containing them 

Solution 1: start with empty heap and insert items one at a time

```
simpleHeapBuilding(A)
A: an array 
    initialize H as an empty heap 
    for i = 0 to size(A) - 1 do 
        H.insert(A[i])
```

This corresponds to doing fix-ups           
Worst-case running time: $\Theta (n \cdot log(n))$

Solution 2: using fix-doesn instead: 
```
heapify(A)
A: an array 
    n = A.size()
    for i = 0 parent(last(n)) downto 0 do 
        fix-down(A, n, i)
```

### HeapSort 
```
HeapSort(A, n) 
    //heapify 
    n = A.size() 
    for i = parent(last(n)) downto 0 do 
        fix-down(A, n, i) 

    // repeatedly find minimum 
    while n > 1 
        // delete maximum 
        swap items at A[root()] and A[last(n)]
        decrease n 
        fix-down(A, n, root())
```
    
## Module 2 Summary

* Binary Heap: A binary tree that satisfies structural and heap-order property
* Heaps are on possible realization of ADT PriorityQueue: 
    - insert takes O(log n)
    - deleteMax takes O(log n)
    - Also supports findMax in time O(1)
* A binary heap can be built in linear time 
* PQ-sort with binary heaps leads to a sorting algorithm with O(n log n) worst-case run-time (HeapSort)
* We have seen here the *max-oriented version* of heaps (the maximum priority is at the root)
    * there exists a summetric min-oriented version that supports insert and deleteMin with the same runtimes 

### Find the smallest item 

Problem: Find the kth smallest item in array A of n distinct numbers 

Solution 1: Make k passes through the array, deleting the minimum number each time 
- O(kn)

Solution 2: Sort A, then return A[k-1]
- O(n log n)

Solution 3: Scan the array and maintain the k smallest numbers seen so far in a max-heap 
- O(n log k)

Solution 4: Create a min-heap with heapify(A). Call deleteMin(A) k times 
- O(n + k log n)




